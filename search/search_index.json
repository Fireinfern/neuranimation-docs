{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Neuranimation Neuranimation is a Unity 3D Package that uses two different approaches to computer animations. Using the state of the art Phase Function Neural Network developed by Holden, for the basic movements that the user can perfom with a character. For the personalized interactions, we present a simple Inverse Kinematic implementations that can be animated in unity using animations.","title":"Home"},{"location":"#welcome-to-neuranimation","text":"Neuranimation is a Unity 3D Package that uses two different approaches to computer animations. Using the state of the art Phase Function Neural Network developed by Holden, for the basic movements that the user can perfom with a character. For the personalized interactions, we present a simple Inverse Kinematic implementations that can be animated in unity using animations.","title":"Welcome to Neuranimation"},{"location":"about/","text":"The Neuranimation Project The Neruanimation Project appears due to the complexity of game animations for independent or small game companies. The objective of the Neuranimation Preoject is to provide a Unity 3D Package to simplify the creation of animations using the Phase-Function Nerual Network for the basic animations. The Team Sebastian Silva - u201611220@upc.edu.pe Sergio Sugahara - u201513410@upc.edu.pe","title":"About"},{"location":"about/#the-neuranimation-project","text":"The Neruanimation Project appears due to the complexity of game animations for independent or small game companies. The objective of the Neuranimation Preoject is to provide a Unity 3D Package to simplify the creation of animations using the Phase-Function Nerual Network for the basic animations.","title":"The Neuranimation Project"},{"location":"about/#the-team","text":"","title":"The Team"},{"location":"about/#sebastian-silva","text":"- u201611220@upc.edu.pe","title":"Sebastian Silva"},{"location":"about/#sergio-sugahara","text":"- u201513410@upc.edu.pe","title":"Sergio Sugahara"},{"location":"installation/","text":"Installation Unity Package Manager The standard installation uses the Unity Package Manager provided by Unity3D. To open the Package Manager for Unity you have to locate your mouse in the Unity3D Menubar. Then a Menu will open, reveling the different windows that you can open. Select the option Package Manager , you will see the next window. Then, click the plus Cloning Github Repo You have to have Git and UnityHub installed in your PC. To clone out repository use the Command Line to get to the desired installation folder. cd C:/DesiredLocation Then, go to the Neuranimation Repository and the HTTP or SSH from the code button. Then use the Command Line to clone the repository: git clone pasteLink cd Neuranimations Then, open Unity Hub and click the Add button. Then select the folder where you cloned the repository.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#unity-package-manager","text":"The standard installation uses the Unity Package Manager provided by Unity3D. To open the Package Manager for Unity you have to locate your mouse in the Unity3D Menubar. Then a Menu will open, reveling the different windows that you can open. Select the option Package Manager , you will see the next window. Then, click the plus","title":"Unity Package Manager"},{"location":"installation/#cloning-github-repo","text":"You have to have Git and UnityHub installed in your PC. To clone out repository use the Command Line to get to the desired installation folder. cd C:/DesiredLocation Then, go to the Neuranimation Repository and the HTTP or SSH from the code button. Then use the Command Line to clone the repository: git clone pasteLink cd Neuranimations Then, open Unity Hub and click the Add button. Then select the folder where you cloned the repository.","title":"Cloning Github Repo"},{"location":"pfnn/","text":"Phase Functioned Neural Network The Phase-Function Neural Network 1 method consist in including a phase value that is responsible for the current stage of the character, while changin the values of the weights and biases of the neural Network. The change of values is determined by a special Phase Function, for Holden et al. 1 and for our version, we use Catmull-Rom Spline Function 2 . In the following sections we will explain the different components of PFNN Catmull-Rom Cubic Spline The Catmull-Rom spline function is used within the model to achieve phases, since this function can obtain a cyclical value in a simple way, having the initial and final points with the same value. The function with only 4 control points was used in this case since they are enough to be able to give a stable and smooth phase. The input values \u200b\u200bfor this function consist of sets of neural network weights, represented as \\(\\beta = \\left\\lbrace\\alpha_ {0},\\alpha_ {1},\\alpha_ {2},\\alpha_ {3} \\right\\rbrace\\) . With this you can define the Catmull-Rom spline function as: \\[ \\theta(p;\\beta) = \\alpha_{k_{1}}\\\\ + w(\\frac{1}{2}\\alpha k_{2} - \\frac{1}{2}\\alpha_{k_{0}} ) \\\\ + w^{2}(\\alpha_{k_{0}} - \\frac{5}{2}\\alpha_{k_{1}}+2 \\alpha_{k_{2}} - \\frac{1}{2} \\alpha_{k_{3}} )\\\\ + w^{3}(\\frac{3}{2}\\alpha_{k_{1}} - \\frac{3}{2}\\alpha_{k_{2}}+ \\frac{1}{2} \\alpha_{k_{3}} - \\frac{1}{2} \\alpha_{k_{0}} ) \\] \\[ w=\\frac{4p}{2\\pi} \\] \\[ k_{n}=[\\frac{4p}{2\\pi}]+ n-1 \\] This function returns a single set of weights, which will be used for learning this set. This definition was described by Holden, et al. 1 as the Phase function. PFNN Layer We name a Layer as PFNN when it is presented in \\(n\\) sets of weights and biases, depending on the number of phases that are needed in the project. For this project, 4 phases or control points are used, previously mentioned in the Catmull-Rom function. These sets are created in the same layers as the values \u200b\u200bof the pesos or \\(w\\) , and of the bias or \\(b\\) . The weights variable will have the following dimensions \\([4, x, y]\\) , such that \\(x\\) is the dimension of the layer's input data, and \\(y\\) are the layer's units. The bias variable has the dimensions \\([4, y]\\) . In each call of this layer, the functions defined in the Catmull-Rom section are used to obtain different sets for each input of the Catmull-Rom Spline function and obtain either the weight or the bias for this call. After this, the weight obtained by the Catmull-Rom function will go through the matrix multiplication function to obtain the weights, and the output goes through the trigger or directly as an output. The implementation of the NFP layer using Keras is as follows: class PhaseLayer ( layers . Layer ): def __init__ ( self , rng = np . random . RandomState ( 23456 ), units = 512 , input_dim = 512 , number_of_phases = 4 ): super ( PhaseLayer , self ) . __init__ () self . nslices = number_of_phases self . units = units self . input_dim = input_dim self . rng = rng self . w = tf . Variable ( self . initial_alpha (), name = \"w\" , trainable = True ) self . b = tf . Variable ( self . initial_beta (), name = \"b\" , trainable = True ) def initial_alpha ( self ): shape = ( self . nslices , self . input_dim , self . units ) alpha_bound = np . sqrt ( 6. / np . prod ( shape [ - 2 :])) alpha = np . asarray ( self . rng . uniform ( low =- alpha_bound , high = alpha_bound , size = shape ), dtype = np . float32 ) return tf . convert_to_tensor ( alpha , dtype = tf . float32 ) def initial_beta ( self ): return tf . zeros (( self . nslices , self . units ), dtype = tf . float32 ) Model Design Activation Function For the current project, the Exponetial Linear Unity or ELU activator will be used, which is defined as: \\[ f(x)=\\left\\{ \\begin{array}{lcc} \\alpha(e^{x}-1), & x < 0 \\\\ x, & x \\geq 0 \\end{array} \\right. \\] Dropout Layer This layer has the probability (indicated by the developer) of deactivating some neurons of the layer in which it is applied, in order to avoid an over-training of the network, assigning 0 as the weight coefficient to the neurons that are deactivated. Model Structure The built model consists of the following layers in the same order: PFNN layer, 311 input data, 512 units. ELU activator Dropout, with a ratio of 0.3 PFNN layer, 512 input data, 512 units ELU activator Dropout, ratio of 0.3 PFNN layer, 512 input data, 304 units The input values \u200b\u200bof the first layer and the units of the last layer can change depending on the amount of activities or inputs that the character has to perform. The code implemented for the modeling of the function is the following: class PFNN ( tf . keras . Model ): def __init__ ( self , input_dim = 1 , output_dim = 1 , dropout = 0.3 , ** kwargs ): super ( PFNN , self ) . __init__ ( ** kwargs ) self . nslices = 4 self . input_dim = input_dim self . output_dim = output_dim self . dropout0 = layers . Dropout ( dropout ) self . dropout1 = layers . Dropout ( dropout ) self . dropout2 = layers . Dropout ( dropout ) self . activation = layers . ELU () self . layer0 = PhaseLayer ( input_dim = input_dim ) self . layer1 = PhaseLayer () self . layer2 = PhaseLayer ( units = output_dim ) def call ( self , inputs ): pscale = self . nslices * inputs [:, - 1 ] pamount = pscale % 1.0 pindex_1 = tf . cast ( pscale , 'int32' ) % self . nslices pindex_0 = ( pindex_1 - 1 ) % self . nslices pindex_2 = ( pindex_1 + 1 ) % self . nslices pindex_3 = ( pindex_1 + 2 ) % self . nslices bamount = tf . expand_dims ( pamount , 1 ) wamount = tf . expand_dims ( bamount , 1 ) def cubic ( y0 , y1 , y2 , y3 , mu ): return ( ( - 0.5 * y0 + 1.5 * y1 - 1.5 * y2 + 0.5 * y3 ) * mu * mu * mu + ( y0 - 2.5 * y1 + 2.0 * y2 - 0.5 * y3 ) * mu * mu + ( - 0.5 * y0 + 0.5 * y2 ) * mu + ( y1 )) W0 = cubic ( tf . nn . embedding_lookup ( self . layer0 . w , pindex_0 ), tf . nn . embedding_lookup ( self . layer0 . w , pindex_1 ), tf . nn . embedding_lookup ( self . layer0 . w , pindex_2 ), tf . nn . embedding_lookup ( self . layer0 . w , pindex_3 ), wamount ) W1 = cubic ( tf . nn . embedding_lookup ( self . layer1 . w , pindex_0 ), tf . nn . embedding_lookup ( self . layer1 . w , pindex_1 ), tf . nn . embedding_lookup ( self . layer1 . w , pindex_2 ), tf . nn . embedding_lookup ( self . layer1 . w , pindex_3 ), wamount ) W2 = cubic ( tf . nn . embedding_lookup ( self . layer2 . w , pindex_0 ), tf . nn . embedding_lookup ( self . layer2 . w , pindex_1 ), tf . nn . embedding_lookup ( self . layer2 . w , pindex_2 ), tf . nn . embedding_lookup ( self . layer2 . w , pindex_3 ), wamount ) b0 = cubic ( tf . nn . embedding_lookup ( self . layer0 . b , pindex_0 ), tf . nn . embedding_lookup ( self . layer0 . b , pindex_1 ), tf . nn . embedding_lookup ( self . layer0 . b , pindex_2 ), tf . nn . embedding_lookup ( self . layer0 . b , pindex_3 ), bamount ) b1 = cubic ( tf . nn . embedding_lookup ( self . layer1 . b , pindex_0 ), tf . nn . embedding_lookup ( self . layer1 . b , pindex_1 ), tf . nn . embedding_lookup ( self . layer1 . b , pindex_2 ), tf . nn . embedding_lookup ( self . layer1 . b , pindex_3 ), bamount ) b2 = cubic ( tf . nn . embedding_lookup ( self . layer2 . b , pindex_0 ), tf . nn . embedding_lookup ( self . layer2 . b , pindex_1 ), tf . nn . embedding_lookup ( self . layer2 . b , pindex_2 ), tf . nn . embedding_lookup ( self . layer2 . b , pindex_3 ), bamount ) H0 = inputs [:, : - 1 ] H1 = self . activation ( tf . matmul ( self . dropout0 ( H0 ), W0 ) + b0 ) H2 = self . activation ( tf . matmul ( self . dropout0 ( H1 ), W1 ) + b1 ) H3 = tf . matmul ( self . dropout2 ( H2 ), W2 ) + b2 return H3 Original Implementation and Data In the original blog by Holden contains the origin version of the code for the PFNN Network using Theano. Here you can also find the data used for out work and th weights and biases of the output. Original Implementation of Holden Holden, D., Komura, T., & Saito, J. (2017). Phase-functioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4), 1-13. \u21a9 \u21a9 \u21a9 Twigg, C. (2003). Catmull-rom splines. Computer, 41(6), 4-6. \u21a9","title":"PFNN"},{"location":"pfnn/#phase-functioned-neural-network","text":"The Phase-Function Neural Network 1 method consist in including a phase value that is responsible for the current stage of the character, while changin the values of the weights and biases of the neural Network. The change of values is determined by a special Phase Function, for Holden et al. 1 and for our version, we use Catmull-Rom Spline Function 2 . In the following sections we will explain the different components of PFNN","title":"Phase Functioned Neural Network"},{"location":"pfnn/#catmull-rom-cubic-spline","text":"The Catmull-Rom spline function is used within the model to achieve phases, since this function can obtain a cyclical value in a simple way, having the initial and final points with the same value. The function with only 4 control points was used in this case since they are enough to be able to give a stable and smooth phase. The input values \u200b\u200bfor this function consist of sets of neural network weights, represented as \\(\\beta = \\left\\lbrace\\alpha_ {0},\\alpha_ {1},\\alpha_ {2},\\alpha_ {3} \\right\\rbrace\\) . With this you can define the Catmull-Rom spline function as: \\[ \\theta(p;\\beta) = \\alpha_{k_{1}}\\\\ + w(\\frac{1}{2}\\alpha k_{2} - \\frac{1}{2}\\alpha_{k_{0}} ) \\\\ + w^{2}(\\alpha_{k_{0}} - \\frac{5}{2}\\alpha_{k_{1}}+2 \\alpha_{k_{2}} - \\frac{1}{2} \\alpha_{k_{3}} )\\\\ + w^{3}(\\frac{3}{2}\\alpha_{k_{1}} - \\frac{3}{2}\\alpha_{k_{2}}+ \\frac{1}{2} \\alpha_{k_{3}} - \\frac{1}{2} \\alpha_{k_{0}} ) \\] \\[ w=\\frac{4p}{2\\pi} \\] \\[ k_{n}=[\\frac{4p}{2\\pi}]+ n-1 \\] This function returns a single set of weights, which will be used for learning this set. This definition was described by Holden, et al. 1 as the Phase function.","title":"Catmull-Rom Cubic Spline"},{"location":"pfnn/#pfnn-layer","text":"We name a Layer as PFNN when it is presented in \\(n\\) sets of weights and biases, depending on the number of phases that are needed in the project. For this project, 4 phases or control points are used, previously mentioned in the Catmull-Rom function. These sets are created in the same layers as the values \u200b\u200bof the pesos or \\(w\\) , and of the bias or \\(b\\) . The weights variable will have the following dimensions \\([4, x, y]\\) , such that \\(x\\) is the dimension of the layer's input data, and \\(y\\) are the layer's units. The bias variable has the dimensions \\([4, y]\\) . In each call of this layer, the functions defined in the Catmull-Rom section are used to obtain different sets for each input of the Catmull-Rom Spline function and obtain either the weight or the bias for this call. After this, the weight obtained by the Catmull-Rom function will go through the matrix multiplication function to obtain the weights, and the output goes through the trigger or directly as an output. The implementation of the NFP layer using Keras is as follows: class PhaseLayer ( layers . Layer ): def __init__ ( self , rng = np . random . RandomState ( 23456 ), units = 512 , input_dim = 512 , number_of_phases = 4 ): super ( PhaseLayer , self ) . __init__ () self . nslices = number_of_phases self . units = units self . input_dim = input_dim self . rng = rng self . w = tf . Variable ( self . initial_alpha (), name = \"w\" , trainable = True ) self . b = tf . Variable ( self . initial_beta (), name = \"b\" , trainable = True ) def initial_alpha ( self ): shape = ( self . nslices , self . input_dim , self . units ) alpha_bound = np . sqrt ( 6. / np . prod ( shape [ - 2 :])) alpha = np . asarray ( self . rng . uniform ( low =- alpha_bound , high = alpha_bound , size = shape ), dtype = np . float32 ) return tf . convert_to_tensor ( alpha , dtype = tf . float32 ) def initial_beta ( self ): return tf . zeros (( self . nslices , self . units ), dtype = tf . float32 )","title":"PFNN Layer"},{"location":"pfnn/#model-design","text":"","title":"Model Design"},{"location":"pfnn/#activation-function","text":"For the current project, the Exponetial Linear Unity or ELU activator will be used, which is defined as: \\[ f(x)=\\left\\{ \\begin{array}{lcc} \\alpha(e^{x}-1), & x < 0 \\\\ x, & x \\geq 0 \\end{array} \\right. \\]","title":"Activation Function"},{"location":"pfnn/#dropout-layer","text":"This layer has the probability (indicated by the developer) of deactivating some neurons of the layer in which it is applied, in order to avoid an over-training of the network, assigning 0 as the weight coefficient to the neurons that are deactivated.","title":"Dropout Layer"},{"location":"pfnn/#model-structure","text":"The built model consists of the following layers in the same order: PFNN layer, 311 input data, 512 units. ELU activator Dropout, with a ratio of 0.3 PFNN layer, 512 input data, 512 units ELU activator Dropout, ratio of 0.3 PFNN layer, 512 input data, 304 units The input values \u200b\u200bof the first layer and the units of the last layer can change depending on the amount of activities or inputs that the character has to perform. The code implemented for the modeling of the function is the following: class PFNN ( tf . keras . Model ): def __init__ ( self , input_dim = 1 , output_dim = 1 , dropout = 0.3 , ** kwargs ): super ( PFNN , self ) . __init__ ( ** kwargs ) self . nslices = 4 self . input_dim = input_dim self . output_dim = output_dim self . dropout0 = layers . Dropout ( dropout ) self . dropout1 = layers . Dropout ( dropout ) self . dropout2 = layers . Dropout ( dropout ) self . activation = layers . ELU () self . layer0 = PhaseLayer ( input_dim = input_dim ) self . layer1 = PhaseLayer () self . layer2 = PhaseLayer ( units = output_dim ) def call ( self , inputs ): pscale = self . nslices * inputs [:, - 1 ] pamount = pscale % 1.0 pindex_1 = tf . cast ( pscale , 'int32' ) % self . nslices pindex_0 = ( pindex_1 - 1 ) % self . nslices pindex_2 = ( pindex_1 + 1 ) % self . nslices pindex_3 = ( pindex_1 + 2 ) % self . nslices bamount = tf . expand_dims ( pamount , 1 ) wamount = tf . expand_dims ( bamount , 1 ) def cubic ( y0 , y1 , y2 , y3 , mu ): return ( ( - 0.5 * y0 + 1.5 * y1 - 1.5 * y2 + 0.5 * y3 ) * mu * mu * mu + ( y0 - 2.5 * y1 + 2.0 * y2 - 0.5 * y3 ) * mu * mu + ( - 0.5 * y0 + 0.5 * y2 ) * mu + ( y1 )) W0 = cubic ( tf . nn . embedding_lookup ( self . layer0 . w , pindex_0 ), tf . nn . embedding_lookup ( self . layer0 . w , pindex_1 ), tf . nn . embedding_lookup ( self . layer0 . w , pindex_2 ), tf . nn . embedding_lookup ( self . layer0 . w , pindex_3 ), wamount ) W1 = cubic ( tf . nn . embedding_lookup ( self . layer1 . w , pindex_0 ), tf . nn . embedding_lookup ( self . layer1 . w , pindex_1 ), tf . nn . embedding_lookup ( self . layer1 . w , pindex_2 ), tf . nn . embedding_lookup ( self . layer1 . w , pindex_3 ), wamount ) W2 = cubic ( tf . nn . embedding_lookup ( self . layer2 . w , pindex_0 ), tf . nn . embedding_lookup ( self . layer2 . w , pindex_1 ), tf . nn . embedding_lookup ( self . layer2 . w , pindex_2 ), tf . nn . embedding_lookup ( self . layer2 . w , pindex_3 ), wamount ) b0 = cubic ( tf . nn . embedding_lookup ( self . layer0 . b , pindex_0 ), tf . nn . embedding_lookup ( self . layer0 . b , pindex_1 ), tf . nn . embedding_lookup ( self . layer0 . b , pindex_2 ), tf . nn . embedding_lookup ( self . layer0 . b , pindex_3 ), bamount ) b1 = cubic ( tf . nn . embedding_lookup ( self . layer1 . b , pindex_0 ), tf . nn . embedding_lookup ( self . layer1 . b , pindex_1 ), tf . nn . embedding_lookup ( self . layer1 . b , pindex_2 ), tf . nn . embedding_lookup ( self . layer1 . b , pindex_3 ), bamount ) b2 = cubic ( tf . nn . embedding_lookup ( self . layer2 . b , pindex_0 ), tf . nn . embedding_lookup ( self . layer2 . b , pindex_1 ), tf . nn . embedding_lookup ( self . layer2 . b , pindex_2 ), tf . nn . embedding_lookup ( self . layer2 . b , pindex_3 ), bamount ) H0 = inputs [:, : - 1 ] H1 = self . activation ( tf . matmul ( self . dropout0 ( H0 ), W0 ) + b0 ) H2 = self . activation ( tf . matmul ( self . dropout0 ( H1 ), W1 ) + b1 ) H3 = tf . matmul ( self . dropout2 ( H2 ), W2 ) + b2 return H3","title":"Model Structure"},{"location":"pfnn/#original-implementation-and-data","text":"In the original blog by Holden contains the origin version of the code for the PFNN Network using Theano. Here you can also find the data used for out work and th weights and biases of the output. Original Implementation of Holden Holden, D., Komura, T., & Saito, J. (2017). Phase-functioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4), 1-13. \u21a9 \u21a9 \u21a9 Twigg, C. (2003). Catmull-rom splines. Computer, 41(6), 4-6. \u21a9","title":"Original Implementation and Data"},{"location":"usage/","text":"Using the Package In the following sections we described the usage of the package. This will include, creating an animation, using the animation and a view on the options and ui components available for the user. Neuranimation Menu The Neuranimation menu is a simple dropdown menu that has different resources to help the user. This Guide will be available in the same Dropdown menu. Neuranimation Window - (Information) The first option availble is the Neuranimation window. In this window we display basic information and some links to out outside resources, like our repository. Versions Option - (Versions) This option will redirect the user to the Git repository. Model Option - (Model) This option will send the user to the model Repository. But if you like to use other PFNN implemetation you can find some in the following pages: https://theorangeduck.com/page/phase-functioned-neural-networks-character-control https://www.starke-consult.de/portfolio/assets/content/work/8/page.html Guide Option - (Guide) The Final Option presents a Link to this guide. Storing Parameters The package can not be run out of the box. First, you need to load the Neuranimation Scene, this scene can be found in the Scenes folder. After you opened the scene. you can see a a character, some objects and terrain. Before you start to walk around with the caracter you will have to store the parameters of the Neural Network. We have included a pretrained version of the model and this can be load with the Store Parameter Button, present in the Pffn_Skeleton. When you press the store Parameter Button you can look in the ModelWB folder a new parameter file. This file will be used by the neural network to create the animations. Now you can press the Play button and start walking. Create a animations To create a new animation or interaction you will have to use the animator an create a new Unity 3D animation. When you have the animation, using the rigs of the character, you can add the interaction to the animation switcher. This is a component we have develop to manage the interactions of characters.","title":"Usage"},{"location":"usage/#using-the-package","text":"In the following sections we described the usage of the package. This will include, creating an animation, using the animation and a view on the options and ui components available for the user.","title":"Using the Package"},{"location":"usage/#neuranimation-menu","text":"The Neuranimation menu is a simple dropdown menu that has different resources to help the user. This Guide will be available in the same Dropdown menu.","title":"Neuranimation Menu"},{"location":"usage/#neuranimation-window-information","text":"The first option availble is the Neuranimation window. In this window we display basic information and some links to out outside resources, like our repository.","title":"Neuranimation Window - (Information)"},{"location":"usage/#versions-option-versions","text":"This option will redirect the user to the Git repository.","title":"Versions Option - (Versions)"},{"location":"usage/#model-option-model","text":"This option will send the user to the model Repository. But if you like to use other PFNN implemetation you can find some in the following pages: https://theorangeduck.com/page/phase-functioned-neural-networks-character-control https://www.starke-consult.de/portfolio/assets/content/work/8/page.html","title":"Model Option - (Model)"},{"location":"usage/#guide-option-guide","text":"The Final Option presents a Link to this guide.","title":"Guide Option - (Guide)"},{"location":"usage/#storing-parameters","text":"The package can not be run out of the box. First, you need to load the Neuranimation Scene, this scene can be found in the Scenes folder. After you opened the scene. you can see a a character, some objects and terrain. Before you start to walk around with the caracter you will have to store the parameters of the Neural Network. We have included a pretrained version of the model and this can be load with the Store Parameter Button, present in the Pffn_Skeleton. When you press the store Parameter Button you can look in the ModelWB folder a new parameter file. This file will be used by the neural network to create the animations. Now you can press the Play button and start walking.","title":"Storing Parameters"},{"location":"usage/#create-a-animations","text":"To create a new animation or interaction you will have to use the animator an create a new Unity 3D animation. When you have the animation, using the rigs of the character, you can add the interaction to the animation switcher. This is a component we have develop to manage the interactions of characters.","title":"Create a animations"}]}